# robots.txt for SifterSearch
# https://siftersearch.com

# Allow all search engines to crawl the site
User-agent: *
Allow: /

# Disallow API endpoints from indexing
Disallow: /api/

# Disallow temporary/internal paths
Disallow: /tmp/
Disallow: /_astro/

# Sitemap locations
# Main sitemap (static pages)
Sitemap: https://siftersearch.com/sitemap-index.xml
# Library documents sitemap (5000+ documents)
Sitemap: https://siftersearch.com/sitemap-library.xml

# Crawl delay (optional, be nice to crawlers)
Crawl-delay: 1
